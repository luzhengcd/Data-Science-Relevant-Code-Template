{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Common distribution   \n",
    "\n",
    "* Bernoulli distribution: the outcome is binary, either 1 or 0. Often, 1 represents success, 0 represents failure. Mathematically The bernoulli distribution is characterized by the success probability of p. If you extend the bernoulli distribution to a more general case where we would have more than two outcomes, you can get a categorical distribution.\n",
    "\n",
    "* Binomial distribution: Distrete probability distribution for exact k success out of n iid bernoulli trials.\n",
    "    EXP: flip a coin for 100 times, what is the probability of 10 of them coming out heads. \n",
    "    \n",
    "    The expection and variance is every easy to estimate for binomial distribution. Binomial distribution is also used to model the click-through-probability in A/B testing where you are interested in how many users click a certain bottom out of all users visited that page. \n",
    " \n",
    " \n",
    "* Geometric distribution: \n",
    "    \n",
    "\n",
    " \n",
    "* Possion distribution: Discrete distribution of the number of iid events happening within a fixed time. For example, it models the number of calls customer service representatives will receive within 1 hour. Say on average, a customer representative would receive 5 calls per hour, then the probability for 5 is very high, and more than 5 calls or less than 5 calls will be relatively rare. So the shape of the\n",
    "\n",
    "* Exponential distribution: A continuous probability distribution of times between events. For a given number of events, we want to know the probability of this number of events happening during different time frame, say 1 hour or 2 hour. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Common descriptive stats\n",
    "\n",
    "* Expectation: the expectation or expected value, is the weighted average value of a random variable.  \n",
    "\n",
    "* Variance: The average of the squared differences from the Mean. It can be used to describe the spread or dispersion of the data. Sometimes, people also use it to represent how much information the data contains. Like in PCA, the top principle components are those direction with large variance.  \n",
    "\n",
    "* What is relationship of the mean and median given a heavily skewed distribution. Note that heavily skewed is in terms of the probability distribution. \n",
    "    If the distribution is  skewed to the left, means that most data concentrate on the right of the distribution. Meaning that the median is larger than the mean.  \n",
    "    \n",
    "    \n",
    "![](pic/mean_median_compare.jpg)\n",
    "\n",
    "\n",
    "* Covariance and corelation. \n",
    "    Both of them measure the Correlation is standardized covariance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. P-value and confidence interval    \n",
    "\n",
    "Wikipedia Explaination: the p-value is the probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.\n",
    "\n",
    "\n",
    "* P-value  \n",
    "\n",
    "null hypothesis  \n",
    "alternate hypothesis  \n",
    "\n",
    "the null hypothesis is the mean of A = the mean of B\n",
    "\n",
    "The two-tail P value is 0.0004: the maxinum chance of observing the mean of the two population is the same. Given such low chance, we can safely say that it's very unlikely to happen, so we reject the null. \n",
    "\n",
    "Or the probability of seeing the observed value by chance. \n",
    "\n",
    "If you set the p value to be 5%, and at 5% level, the critical value is a, and if the value you get is less larger than critical value, meaning that the chance can be only less than 5%. \n",
    "\n",
    "\n",
    "* What is confidence interval:\n",
    "\n",
    "95% confidence interval:\n",
    "\n",
    "Many people have misunderstanding on confidence interval. Say we have 95% confidence interval, the incorrect explaination is the there is 95% of chance that the true mean will fall between this interval. \n",
    "\n",
    "The correct explaination is if you estimate the mean for 100 times, and you'll get 100 confidence interval based on the data, and 95 of the interval will contain the true mean. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Common hypothesis testing   \n",
    "\n",
    "T-test: compare the mean when sample size is small: the variance is unknown, need to use current data to estimate one\n",
    "\n",
    "Z-test: compare the mean when you have a large sample size so that you can assume it follows normal distribution: the variance is known  \n",
    "\n",
    "\n",
    "SEM: standard error of the mean!!! When the population goes to infinity, the SEM tend to be 0.\n",
    "\n",
    "One sample t-test: if the mean equals to a exact number\n",
    "two sample t-test: if the mean of two samples are the same\n",
    "\n",
    "paired t-test: the difference between paired t-test and two-sample test in [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5579465/#:~:text=Two%2Dsample%20t%2Dtest%20is%20used%20when%20the%20data%20of,some%20technical%20differences%20between%20them.).  \n",
    "\n",
    "For paired samples that are correlated with each other. If two observations within the same pair are positively (negatively) correlated, i.e. ρ> 0(< 0), the variance of the mean difference is smaller (larger) than that in the case of independent samples. They are equal if two samples are uncorrelated (ρ= 0).  \n",
    "\n",
    "Technical differenct:\n",
    "    1. For two-sample t-test, it assume that the sample are drawn from two iid normal distribution with the same variance (unknow).  \n",
    "    2. Paired t-test only requires the difference follows normal distribution. \n",
    "    \n",
    "Paired sample (time related sample): the same patient the symptom score pre-treatment and post-treatment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. OLS\n",
    "\n",
    "Why the assumption has to be met: the parameters we estimated using OLS is unbiased with smallest variance. \n",
    "\n",
    "Assumption:  \n",
    "\n",
    "1. Linearity: the feature and response have to be linearly related.  \n",
    "2. Multicollinearity(Can use VIF): The features are not corrleated with each other  \n",
    "    The interpretation of a regression coefficient is that it represents the mean change in the target for each unit change in an feature when you hold all of the other features constant. However, when features are correlated, changes in one feature in turn shifts another feature/features. The stronger the correlation, the more difficult it is to change one feature without changing another. It becomes difficult for the model to estimate the relationship between each feature and the target independently because the features tend to change in unison.\n",
    "\n",
    "    How to fix: \n",
    "        remove one variable / PCA\n",
    "3. Heteroscedasticity (plot out the vairable and observe): The variance for a variable is constant.  \n",
    "    Why: Recall that ordinary least-squares (OLS) regression seeks to minimize residuals and in turn produce the smallest possible standard errors.  By definition, OLS regression gives equal weight to all observations, but when heteroscedasticity is present, the cases with larger disturbances have more “pull” than other observations.  In this case, weighted least squares regression would be more appropriate, as it down-weights those observations with larger disturbances.\n",
    "\n",
    "    How to fix it:\n",
    "        weighted least square\n",
    "        transform Y: ln(Y), sqrt(Y), 1/Y\n",
    "\n",
    "4. Normality: (q-q plot)\n",
    "    If residual does not follow normal distribution, when make prediction, the confidence interval may not be accurate, because it’s calculated under the assumption of normality. Also MLE lost its efficiency, cus least square is based on MLE. One thing about normality, if the residual do not follow a normal distribution, p value may not reliable, because essentially p value is from a normal distribution.  \n",
    "    \n",
    "    How to fix it: \n",
    "        Box-cox transformation to transform it to normal.\n",
    "\n",
    "What if the assumptions are violated: \n",
    "* The hypothesis testing results are no longer trustable\n",
    "* The estimation of the coefficient is biased and it does not has smallest variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity can affect any regression model with more than one predictor. It occurs when two or more predictor variables overlap so much in what they measure that their effects are indistinguishable. \n",
    "\n",
    "How to check multicollinearity: \n",
    "    * The most intuitive way is plotting out the correlation matrix, and see which two have a correlation coefficient larger than say .8, or .9. It may miss the case where more than two variables correlate with each other.   \n",
    "    * Use VIF, variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated. If no factors are correlated, the VIFs will all be 1.\n",
    "    * The overall model is significant, but none of the coefficients are\n",
    "Remember that a p-value for a coefficient tests whether the unique effect of that predictor on Y is zero. If all predictors overlap in what they measure, there is little unique effect, even if the predictors as a group have an effect on Y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing in OLS:  \n",
    "\n",
    "* F-test: test the overall significance of the model. The null hypothesis is all the coefficient are zero. If we reject the null, meaning at least one of the coefficient is significant\n",
    "\n",
    "* T-test: test the significance of an unique coeficient. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Central limit theorem\n",
    "\n",
    "the distribution of the sample means approaches a normal distribution as the sample size gets larger — no matter what the shape of the population distribution. This fact holds especially true for sample sizes over 30.\n",
    "\n",
    "Dice example: if you roll a dice for 10 times, and get the mean, and roll the dice for another times times, and get the mean, and keep rolling it this way. \n",
    "\n",
    "Why central limit theorem is useful.\n",
    "\n",
    "When you do hypothesis testing, like t-test or z-test for the mean, we use the assumption that the mean follow a normal distribution, and that assumption comes from central limit theorem. \n",
    "\n",
    "Also, when you estimate the variance of the mean, we know the rule of thumb is, the variance is proportional to 1/sqrt(n) where n is the number of data points. When the sample size is large enough, we can expect the distribution of mean to be a tighter. \n",
    "\n",
    "\n",
    "### 6. The law of large number \n",
    "\n",
    "According to the law, the average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. A/B testing\n",
    "\n",
    "Power: power = $1-\\beta$, beta is commonly set to be .2. \n",
    "\n",
    "* Power (1-$\\beta$): the probability that the test correctly rejects the null hypothesis. \n",
    "\n",
    "* Significance level $\\alpha$:  the probability of rejecting the null hypothesis if it were true. That is the probability of **making a Type I Error** or a **false positive**. \n",
    "\n",
    "* Effect size: the standard difference between two groups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Is mean imputation of missing data acceptable practice? Why or why not?\n",
    "\n",
    "* No  \n",
    "* Leads to an underestimate of the standard deviation\n",
    "* Distorts relationships between variables by “pulling” estimates of the correlation toward zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How do you handle missing data? What imputation techniques do you recommend?\n",
    "If data missing at random: deletion has no bias effect, but decreases the power of the analysis by decreasing the effective sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. You’re about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it’s raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that “Yes” it is raining. What is the probability that it’s actually raining in Seattle?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ANOVA analysis  \n",
    "\n",
    "When comparing two samples, using t-test or ANOVA give you the same results. But if you have multiple samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
